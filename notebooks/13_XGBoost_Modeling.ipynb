{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dc848b-f570-49a8-9ae1-8ddb2ca11d48",
   "metadata": {},
   "source": [
    "# 13. XGBoost Modeling and Final Comparison\n",
    "\n",
    "This notebook focuses on applying the powerful XGBoost algorithm to our three city datasets (Berlin, Istanbul, and Munich). We will compare its performance against our previous Random Forest models to determine if it can improve on the results, especially for Istanbul and Munich. This analysis represents the final iteration of our project's modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff9997f-68d9-4126-bf99-31fa026751bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Add the parent directory (utils folder) to the system path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Import our custom data loading and model utility functions\n",
    "from utils.data_loader import load_and_clean_data\n",
    "from utils.model_utils import prepare_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa2b3ba-c299-4227-a87c-e367c3238408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_xgboost_model(city_name):\n",
    "    \"\"\"\n",
    "    Loads data for a given city, prepares it, trains an XGBoost model,\n",
    "    and evaluates its performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting XGBoost Modeling for {city_name} ---\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    df_city = load_and_clean_data(city_name)\n",
    "    df_city.drop(columns=['host_since', 'calendar_last_scraped', 'first_review', 'last_review'], errors='ignore', inplace=True)\n",
    "    X, y = prepare_features(df_city, target_column='price')\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # --- HATA DÜZELTME: XGBoost için özellik isimlerini temizleme ---\n",
    "    # [] ve < gibi karakterleri temizler\n",
    "    X.columns = X.columns.str.replace('[\\[\\]<]', '', regex=True)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the XGBoost Regressor model\n",
    "    xg_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training XGBoost model for {city_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    xg_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xg_model.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\n--- XGBoost Model Performance for {city_name} ---\")\n",
    "    print(f\"R-squared (R²) score: {r2:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "    return r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11593b0-a256-4e5a-87c2-ff84fbced084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting XGBoost Modeling for berlin ---\n",
      "Loading cleaned data for Berlin from processed directory...\n",
      "Categorical features have been one-hot encoded.\n",
      "Shape of features (X) after encoding: (9135, 8936)\n",
      "Training XGBoost model for berlin...\n",
      "\n",
      "--- XGBoost Model Performance for berlin ---\n",
      "R-squared (R²) score: 0.7264\n",
      "Root Mean Squared Error (RMSE): 49.01\n",
      "\n",
      "--- Starting XGBoost Modeling for istanbul ---\n",
      "Loading cleaned data for Istanbul from processed directory...\n",
      "Categorical features have been one-hot encoded.\n",
      "Shape of features (X) after encoding: (3340, 3524)\n",
      "Training XGBoost model for istanbul...\n",
      "\n",
      "--- XGBoost Model Performance for istanbul ---\n",
      "R-squared (R²) score: 0.3368\n",
      "Root Mean Squared Error (RMSE): 174.30\n",
      "\n",
      "--- Starting XGBoost Modeling for munich ---\n",
      "Loading cleaned data for Munich from processed directory...\n",
      "Categorical features have been one-hot encoded.\n",
      "Shape of features (X) after encoding: (4687, 4871)\n",
      "Training XGBoost model for munich...\n",
      "\n",
      "--- XGBoost Model Performance for munich ---\n",
      "R-squared (R²) score: 0.5598\n",
      "Root Mean Squared Error (RMSE): 94.24\n"
     ]
    }
   ],
   "source": [
    "# Run the model for Berlin\n",
    "berlin_r2, berlin_rmse = train_and_evaluate_xgboost_model('berlin')\n",
    "\n",
    "# Run the model for Istanbul\n",
    "istanbul_r2, istanbul_rmse = train_and_evaluate_xgboost_model('istanbul')\n",
    "\n",
    "# Run the model for Munich\n",
    "munich_r2, munich_rmse = train_and_evaluate_xgboost_model('munich')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2b3f3-e71d-420f-8f21-d70714b83a67",
   "metadata": {},
   "source": [
    "# Airbnb Price Prediction Case Study\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project is a comprehensive machine learning case study focused on predicting Airbnb prices in three major European cities: Berlin, Istanbul, and Munich. The primary goal was to explore how different machine learning models perform on varying urban market data and to develop a robust, high-performance solution.\n",
    "\n",
    "The project followed an iterative workflow:\n",
    "\n",
    "1.  **Data Preprocessing and Exploratory Data Analysis (EDA):** I began by cleaning and preparing the raw data, followed by a detailed EDA to understand the key factors influencing price in each city.\n",
    "2.  **Baseline Modeling:** A simple Linear Regression model (V1) was established as a baseline to set a benchmark for performance.\n",
    "3.  **Advanced Modeling:** I transitioned to more powerful tree-based ensemble models, starting with a Random Forest Regressor (V2).\n",
    "4.  **Model Optimization:** I performed a hyperparameter tuning process to find the optimal settings for the Random Forest model (V3).\n",
    "5.  **Advanced Algorithm Implementation:** Recognizing that the Random Forest model might have reached its performance limit, especially in Istanbul and Munich, I implemented and evaluated the more advanced XGBoost algorithm.\n",
    "\n",
    "## Final Model Performance Summary: The Ultimate Comparison\n",
    "\n",
    "This table represents the culmination of all modeling efforts, providing a clear comparison of each model's performance.\n",
    "\n",
    "| Model | City | R² Score | RMSE |\n",
    "|---|---|---|---|\n",
    "| **Linear Regression (V1)** | Berlin | 0.008 | 93.31 |\n",
    "| | Istanbul | 0.022 | 211.71 |\n",
    "| | Munich | -0.002 | 142.19 |\n",
    "| **Random Forest (V2/V3)** | Berlin | 0.7133 | 50.17 |\n",
    "| | Istanbul | 0.3140 | 177.27 |\n",
    "| | Munich | 0.4684 | 103.57 |\n",
    "| **XGBoost (Final Model)** | **Berlin** | **0.7264** | **49.01** |\n",
    "| | **Istanbul** | **0.3368** | **174.30** |\n",
    "| | **Munich** | **0.5598** | **94.24** |\n",
    "\n",
    "## Key Insights and Conclusions\n",
    "\n",
    "Through this project, I gained several key insights and demonstrated core data science competencies:\n",
    "\n",
    "-   **Algorithmic Choice Matters:** The jump in performance from Linear Regression (V1) to the ensemble models (Random Forest and XGBoost) was dramatic, confirming that a simple model is often insufficient for complex, non-linear data.\n",
    "-   **Validation and Optimization:** My hyperparameter tuning of the Random Forest model (V3) showed that the initial default model was already performing near its peak.\n",
    "-   **Knowing When to Iterate:** When the Random Forest model's performance plateaued, I chose to implement XGBoost, a superior algorithm for this type of data. This decision led to significant performance gains in Istanbul and Munich, validating my hypothesis that a more powerful model was needed to capture the complexities of those markets.\n",
    "-   **Data-Driven Problem Solving:** I encountered and resolved technical challenges, such as a `ValueError` caused by special characters in feature names, demonstrating my ability to diagnose and fix real-world coding issues.\n",
    "-   **Market-Specific Dynamics:** The final results show that while my models perform exceptionally well in Berlin, the lower scores in Istanbul and Munich suggest that each city has unique market factors not captured by the current dataset, which is a valuable business insight for future work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
